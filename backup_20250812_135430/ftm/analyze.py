#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Bem Vindo ao F.T.M - Follow The Money - Vers√£o 0.2 Beta
Escrito em Python 3
Autor: Thiago Oliveira Castro Vieira - thiago@thiagovieira.adv.br

O objetivo desse software √© reunir informa√ß√µes dispon√≠veis publicamente que
possam levar a identifica√ß√£o da autoria de um site.
Atualmente o software busca pelas seguintes informa√ß√µes: √çP do servidores do
dom√≠nio e subdom√≠nios (tentativa e erro); whois do dom√≠nio e dos servidores;
c√≥digos de identifica√ß√£o do Google (Ad Sense, Analitycs e Sites), Bing,
Juicy AD (Propaganda em Sites Pornogr√°ficos - Canad√°); e links
constantes no site.

Modo de Usar: python ftm-02-Beta.py dom√≠nio (Caso n√£o informe o dom√≠nio na
linha de comando, o software perguntar√° qual o alvo)

RoadMap: a) melhorar o whois do dom√≠nio - n√£o funciona com .br;
b) n√£o ser bloqueado pelo CloudFlare;
c) Exportar um relat√≥rio em PDF.

"""
import socket
import ssl
import dns.resolver
import re
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import urllib.parse
import time
import random
from bs4 import BeautifulSoup
import texttable as tt
import sys
import os
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
from ipwhois import IPWhois
from time import ctime
from ntplib import NTPClient, NTPException
import gzip
import datetime
import pytz

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('ftm_analysis.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

# Importa python-whois (biblioteca mais est√°vel)
try:
    import whois
    WHOIS_AVAILABLE = True
    logger.info("‚úì Python-whois dispon√≠vel - funcionalidade WHOIS ativada")
except ImportError:
    WHOIS_AVAILABLE = False
    logger.warning("‚ö† Python-whois n√£o instalado. Para funcionalidade WHOIS, instale com: pip install python-whois")



def ntp_time(servers):
    """
    Retorna a hora oficial do Brasil (NTP.br) no fuso hor√°rio correto

    Returns the official time of Brazil (NTP.br) in the correct timezone.
    
    Args:
        servers (list): Lista de servidores NTP para consulta
        
    Returns:
        str: Hor√°rio oficial do Brasil formatado ou hor√°rio local em caso de falha
    """
    ntp_time = None
    client = NTPClient()

    for host in servers:
        try:
            response = client.request(host)
            # Converte para datetime e aplica fuso hor√°rio do Brasil
            utc_time = datetime.datetime.fromtimestamp(response.orig_time, tz=datetime.timezone.utc)
            # Fuso hor√°rio de Bras√≠lia (UTC-3)
            brasilia_tz = pytz.timezone('America/Sao_Paulo')
            local_time = utc_time.astimezone(brasilia_tz)
            ntp_time = local_time.strftime('%Y-%m-%d %H:%M:%S %Z')
            break
        except (NTPException, socket.gaierror) as e:
            # Log do erro para diagn√≥stico
            logger.debug(f"Erro ao conectar com servidor NTP {host}: {str(e)}")
            pass
        except Exception as e:
            logger.warning(f"Erro inesperado com servidor NTP {host}: {str(e)}")
            pass

    if not ntp_time:
        # Fallback para hor√°rio local se NTP falhar
        brasilia_tz = pytz.timezone('America/Sao_Paulo')
        local_time = datetime.datetime.now(brasilia_tz)
        ntp_time = f"{local_time.strftime('%Y-%m-%d %H:%M:%S %Z')} (hor√°rio local - NTP indispon√≠vel)"
    
    return ntp_time


def findservidor(dominio_analisado, dicsubdominios):
    """ 
    Busca por subdom√≠nios usando paraleliza√ß√£o para melhor performance.
    
    Args:
        dominio_analisado (str): O dom√≠nio principal para buscar subdom√≠nios
        dicsubdominios (dict): Dicion√°rio para armazenar os subdom√≠nios encontrados
        
    Returns:
        dict: Dicion√°rio com subdom√≠nios encontrados {subdominio: ip}
    """
    subdominios = (
        'www',
        'mail',
        'ftp',
        'cpanel',
        'blog',
        'direct',
        'direct-connect',
        'admin',
        'pop',
        'imap',
        'forum',
        'portal',
        'smtp',
        'm',
        'www2',
        'dev',
        'remote',
        'server',
        'webmail',
        'ns1',
        'ns2',
        'secure',
        'vpn',
        'shop',
        'mail2',
        'teste',
        'ns',
        'host',
        '1',
        '2',
        'mx1',
        'exchange',
        'api',
        'news',
        'vps',
        'gov',
        'owa',
        'cloud',
    )
    
    def resolve_subdomain(sub):
        """Resolve um subdom√≠nio espec√≠fico"""
        try:
            ip = socket.gethostbyname(sub + '.' + dominio_analisado)
            logger.debug(f"Subdom√≠nio encontrado: {sub}.{dominio_analisado} -> {ip}")
            return sub, ip
        except socket.gaierror:
            return sub, None
        except Exception as e:
            logger.warning(f"Erro ao resolver {sub}.{dominio_analisado}: {str(e)}")
            return sub, None
    
    # Usar ThreadPoolExecutor para paralelizar as consultas DNS
    logger.info(f"Iniciando varredura de subdom√≠nios para {dominio_analisado}")
    with ThreadPoolExecutor(max_workers=10) as executor:
        # Submeter todas as tarefas
        future_to_sub = {executor.submit(resolve_subdomain, sub): sub for sub in subdominios}
        
        # Coletar resultados
        for future in as_completed(future_to_sub):
            sub, ip = future.result()
            if ip:
                dicsubdominios[sub] = ip
    
    logger.info(f"Varredura conclu√≠da. {len(dicsubdominios)} subdom√≠nios encontrados")
    return dicsubdominios


def d_whois(dominio_analisado):
    """Consulta informa√ß√µes WHOIS do dom√≠nio com tratamento robusto de erros"""
    if not WHOIS_AVAILABLE:
        logger.warning(f"Tentativa de consulta WHOIS para {dominio_analisado} sem biblioteca dispon√≠vel")
        return f"WHOIS para {dominio_analisado}:\nFuncionalidade WHOIS n√£o dispon√≠vel. Instale python-whois: pip install python-whois"
    
    try:
        # Limpa o dom√≠nio para consulta
        dominio_limpo = dominio_analisado
        if dominio_limpo.startswith(('http://', 'https://')):
            dominio_limpo = dominio_limpo.split('://')[1]
        if dominio_limpo.startswith('www.'):
            dominio_limpo = dominio_limpo[4:]
        if '/' in dominio_limpo:
            dominio_limpo = dominio_limpo.split('/')[0]
        
        logger.info(f"Consultando WHOIS para: {dominio_limpo}")
        
        # Consulta WHOIS
        domain_info = whois.whois(dominio_limpo)
        
        if not domain_info:
            return f"WHOIS para {dominio_analisado}:\nNenhuma informa√ß√£o WHOIS encontrada."
        
        # Fun√ß√£o auxiliar para formatar dados
        def format_data(data):
            if not data:
                return "N√£o informado"
            if isinstance(data, list):
                return ', '.join(str(item) for item in data if item)
            return str(data)
        
        def format_date(date_data):
            if not date_data:
                return "N√£o informado"
            if isinstance(date_data, list):
                return str(date_data[0]) if date_data else "N√£o informado"
            return str(date_data)
        
        # Monta informa√ß√µes organizadas
        whois_info = []
        whois_info.append(f"üìã INFORMA√á√ïES WHOIS - {dominio_analisado.upper()}")
        whois_info.append("‚ïê" * 60)
        
        # üè¢ INFORMA√á√ïES B√ÅSICAS DO DOM√çNIO
        whois_info.append("\nüè¢ INFORMA√á√ïES B√ÅSICAS")
        whois_info.append("‚îÄ" * 30)
        
        domain_name = format_data(domain_info.domain_name)
        if domain_name != "N√£o informado":
            whois_info.append(f"‚Ä¢ Dom√≠nio: {domain_name.upper()}")
        
        registrar = format_data(domain_info.registrar)
        whois_info.append(f"‚Ä¢ Registrador: {registrar}")
        
        if hasattr(domain_info, 'registrar_url') and domain_info.registrar_url:
            whois_info.append(f"‚Ä¢ URL do Registrador: {domain_info.registrar_url}")
        
        whois_server = getattr(domain_info, 'whois_server', None)
        if whois_server:
            whois_info.append(f"‚Ä¢ Servidor WHOIS: {whois_server}")
        
        # üìÖ DATAS IMPORTANTES
        whois_info.append("\nüìÖ DATAS IMPORTANTES")
        whois_info.append("‚îÄ" * 30)
        
        creation_date = format_date(domain_info.creation_date)
        whois_info.append(f"‚Ä¢ Data de Cria√ß√£o: {creation_date}")
        
        expiration_date = format_date(domain_info.expiration_date)
        whois_info.append(f"‚Ä¢ Data de Expira√ß√£o: {expiration_date}")
        
        updated_date = format_date(domain_info.updated_date)
        whois_info.append(f"‚Ä¢ √öltima Atualiza√ß√£o: {updated_date}")
        
        # üîí STATUS E CONFIGURA√á√ïES
        whois_info.append("\nüîí STATUS E CONFIGURA√á√ïES")
        whois_info.append("‚îÄ" * 30)
        
        status = format_data(domain_info.status)
        whois_info.append(f"‚Ä¢ Status: {status}")
        
        name_servers = format_data(domain_info.name_servers)
        if name_servers != "N√£o informado":
            whois_info.append(f"‚Ä¢ Servidores DNS: {name_servers.upper()}")
        
        dnssec = getattr(domain_info, 'dnssec', None)
        if dnssec:
            whois_info.append(f"‚Ä¢ DNSSEC: {dnssec}")
        
        # üë§ INFORMA√á√ïES DO REGISTRANTE
        registrant_info = []
        
        # Nome do registrante
        registrant_name = getattr(domain_info, 'registrant_name', None) or getattr(domain_info, 'name', None)
        if registrant_name:
            registrant_info.append(f"‚Ä¢ Nome: {registrant_name}")
        
        # Organiza√ß√£o
        org = getattr(domain_info, 'org', None)
        if org:
            registrant_info.append(f"‚Ä¢ Organiza√ß√£o: {org}")
        
        # Endere√ßo
        address = getattr(domain_info, 'address', None)
        if address:
            addr_formatted = format_data(address)
            registrant_info.append(f"‚Ä¢ Endere√ßo: {addr_formatted}")
        
        # Cidade, Estado, CEP
        city = getattr(domain_info, 'city', None)
        if city:
            registrant_info.append(f"‚Ä¢ Cidade: {city}")
        
        state = getattr(domain_info, 'state', None)
        if state:
            registrant_info.append(f"‚Ä¢ Estado: {state}")
        
        zipcode = getattr(domain_info, 'zipcode', None)
        if zipcode:
            registrant_info.append(f"‚Ä¢ CEP: {zipcode}")
        
        country = getattr(domain_info, 'country', None)
        if country:
            registrant_info.append(f"‚Ä¢ Pa√≠s: {country}")
        
        if registrant_info:
            whois_info.append("\nüë§ INFORMA√á√ïES DO REGISTRANTE")
            whois_info.append("‚îÄ" * 30)
            whois_info.extend(registrant_info)
        
        # üìû INFORMA√á√ïES DE CONTATO
        contact_info = []
        
        emails = getattr(domain_info, 'emails', None)
        if emails:
            emails_formatted = format_data(emails)
            contact_info.append(f"‚Ä¢ E-mails: {emails_formatted}")
        
        phone = getattr(domain_info, 'phone', None)
        if phone:
            contact_info.append(f"‚Ä¢ Telefone: {phone}")
        
        fax = getattr(domain_info, 'fax', None)
        if fax:
            contact_info.append(f"‚Ä¢ Fax: {fax}")
        
        if contact_info:
            whois_info.append("\nüìû INFORMA√á√ïES DE CONTATO")
            whois_info.append("‚îÄ" * 30)
            whois_info.extend(contact_info)
        
        # üë®‚Äçüíº CONTATO ADMINISTRATIVO
        admin_info = []
        admin_name = getattr(domain_info, 'admin_name', None)
        if admin_name:
            admin_info.append(f"‚Ä¢ Nome: {admin_name}")
            
            admin_org = getattr(domain_info, 'admin_org', None)
            if admin_org:
                admin_info.append(f"‚Ä¢ Organiza√ß√£o: {admin_org}")
            
            admin_email = getattr(domain_info, 'admin_email', None)
            if admin_email:
                admin_info.append(f"‚Ä¢ E-mail: {admin_email}")
            
            admin_phone = getattr(domain_info, 'admin_phone', None)
            if admin_phone:
                admin_info.append(f"‚Ä¢ Telefone: {admin_phone}")
        
        if admin_info:
            whois_info.append("\nüë®‚Äçüíº CONTATO ADMINISTRATIVO")
            whois_info.append("‚îÄ" * 30)
            whois_info.extend(admin_info)
        
        # üîß CONTATO T√âCNICO
        tech_info = []
        tech_name = getattr(domain_info, 'tech_name', None)
        if tech_name:
            tech_info.append(f"‚Ä¢ Nome: {tech_name}")
            
            tech_org = getattr(domain_info, 'tech_org', None)
            if tech_org:
                tech_info.append(f"‚Ä¢ Organiza√ß√£o: {tech_org}")
            
            tech_email = getattr(domain_info, 'tech_email', None)
            if tech_email:
                tech_info.append(f"‚Ä¢ E-mail: {tech_email}")
            
            tech_phone = getattr(domain_info, 'tech_phone', None)
            if tech_phone:
                tech_info.append(f"‚Ä¢ Telefone: {tech_phone}")
        
        if tech_info:
            whois_info.append("\nüîß CONTATO T√âCNICO")
            whois_info.append("‚îÄ" * 30)
            whois_info.extend(tech_info)
        
        # üìÑ INFORMA√á√ïES T√âCNICAS DETALHADAS (apenas se necess√°rio)
        if hasattr(domain_info, 'text') and domain_info.text and len(domain_info.text) > 0:
            whois_info.append("\nüìÑ INFORMA√á√ïES T√âCNICAS DETALHADAS")
            whois_info.append("‚îÄ" * 30)
            
            # domain_info.text pode ser uma lista de strings ou uma string √∫nica
            if isinstance(domain_info.text, list):
                # Se √© uma lista, junta as strings
                raw_text = '\n'.join(str(item) for item in domain_info.text)
            else:
                # Se √© uma string √∫nica
                raw_text = str(domain_info.text)
            
            # Processa as linhas do texto bruto
            lines = raw_text.split('\n')
            relevant_lines = []
            
            for line in lines:
                line = line.strip()
                # Filtra linhas vazias e coment√°rios
                if line and not line.startswith('%') and not line.startswith('#'):
                    relevant_lines.append(f"  {line}")
            
            # Limita o n√∫mero de linhas para melhor legibilidade
            if len(relevant_lines) > 40:
                whois_info.extend(relevant_lines[:40])
                whois_info.append("  ... (informa√ß√µes adicionais omitidas para melhor legibilidade)")
            else:
                whois_info.extend(relevant_lines)
        
        whois_info.append("\n" + "‚ïê" * 60)
        whois_info.append("‚úÖ Consulta WHOIS conclu√≠da com sucesso")
        
        return '\n'.join(whois_info)
        
    except Exception as e:
        error_msg = str(e)
        if "No whois server" in error_msg:
            return f"‚ùå WHOIS para {dominio_analisado}:\n\nüö´ Servidor WHOIS n√£o encontrado para este dom√≠nio.\n\n‚ÑπÔ∏è Alguns dom√≠nios podem n√£o ter informa√ß√µes WHOIS p√∫blicas dispon√≠veis."
        elif "connection" in error_msg.lower():
            return f"‚ùå WHOIS para {dominio_analisado}:\n\nüåê Erro de conex√£o com servidor WHOIS.\n\n‚ÑπÔ∏è Verifique sua conex√£o com a internet e tente novamente."
        else:
            return f"‚ùå WHOIS para {dominio_analisado}:\n\n‚ö†Ô∏è Erro ao consultar WHOIS: {error_msg}\n\nüîß Detalhes t√©cnicos: {str(e)}"


def ip_whois(dicsubdominios):
    """
    Realiza consulta WHOIS para os IPs dos subdom√≠nios encontrados.
    
    Args:
        dicsubdominios (dict): Dicion√°rio com subdom√≠nios e seus IPs
        
    Returns:
        dict: Dicion√°rio com informa√ß√µes WHOIS formatadas para cada subdom√≠nio
    """
    result = {}
    for (k, v) in dicsubdominios.items():
        try:
            hosts = IPWhois(v)  # .lookup_rws()
            results = hosts.lookup_whois()
            tab_nets = tt.Texttable()

            contato_nets = [[]]  # The empty row will have the header

            for i in results['nets'][0].keys():
                contato_nets.append([i, results['nets'][0][i]])

            tab_nets.add_rows(contato_nets)
            tab_nets.set_cols_align(['c', 'c'])
            tab_nets.header(['Dados do Host', '  '])
            result[k] = (tab_nets.draw() + 2 * '\n')
        except Exception as e:
            # Em caso de erro no WHOIS, adiciona uma mensagem informativa
            result[k] = f"Erro ao obter WHOIS para {v}: {str(e)}\n\n"
    return result


def get_ids(url):
    """Extrai c√≥digos de identifica√ß√£o, links, tecnologias e informa√ß√µes de contato de uma p√°gina web usando requests"""
    ids = []
    links = []
    social_links = []
    external_links = []
    internal_links = []
    contact_info = []
    technologies = []
    
    try:
        # Normaliza a URL
        if not url.startswith(('http://', 'https://')):
            url = 'http://' + url
            
        logger.info(f"Iniciando an√°lise de {url}")
            
        # Lista de User-Agents para tentar contornar bloqueios
        user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0'
        ]
        
        # Headers mais completos para simular um navegador real
        headers = {
            'User-Agent': random.choice(user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',
            'Accept-Language': 'pt-BR,pt;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Cache-Control': 'max-age=0',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Pragma': 'no-cache'
        }
        
        # Configurar sess√£o requests com retry e timeout
        session = requests.Session()
        session.headers.update(headers)
        
        # Configurar retry strategy
        
        retry_strategy = Retry(
            total=3,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount("http://", adapter)
        session.mount("https://", adapter)
        
        # Fun√ß√£o para tentar acessar a URL com requests
        def try_access_url_requests(current_url, is_retry=False):
            try:
                # Adiciona um atraso aleat√≥rio para parecer mais humano
                if is_retry:
                    time.sleep(random.uniform(1, 3))
                
                response = session.get(
                    current_url, 
                    timeout=(10, 30),  # (connect timeout, read timeout)
                    allow_redirects=True,
                    verify=True  # Verificar certificados SSL
                )
                response.raise_for_status()  # Levanta exce√ß√£o para c√≥digos de erro HTTP
                
                # Lidar com compress√£o Brotli manualmente se necess√°rio
                content_encoding = response.headers.get('content-encoding', '').lower()
                if content_encoding == 'br':
                    try:
                        import brotli
                        decompressed_content = brotli.decompress(response.content)
                        html_text = decompressed_content.decode('utf-8', errors='ignore')
                        logger.info(f"Brotli decompression successful, HTML length: {len(html_text)}")
                        return html_text
                    except ImportError:
                        logger.warning("Brotli library not available, trying standard decoding")
                    except Exception as e:
                        logger.error(f"Brotli decompression failed: {str(e)}")
                
                # Detectar encoding automaticamente
                response.encoding = response.apparent_encoding or 'utf-8'
                return response.text
                
            except requests.exceptions.SSLError as e:
                logger.warning(f"Erro SSL em {current_url}: {str(e)}")
                # Tentar sem verifica√ß√£o SSL como fallback
                try:
                    response = session.get(
                        current_url, 
                        timeout=(10, 30),
                        allow_redirects=True,
                        verify=False
                    )
                    response.raise_for_status()
                    response.encoding = response.apparent_encoding or 'utf-8'
                    logger.warning(f"Acesso realizado sem verifica√ß√£o SSL para {current_url}")
                    return response.text
                except Exception as e2:
                    logger.error(f"Falha total de SSL para {current_url}: {str(e2)}")
                    raise e
            except Exception as e:
                if not is_retry:
                    # Tenta com outro User-Agent em caso de falha
                    session.headers.update({'User-Agent': random.choice(user_agents)})
                    return try_access_url_requests(current_url, True)
                raise e
        
        # Tenta acessar com HTTP primeiro
        try:
            html = try_access_url_requests(url)
            soup = BeautifulSoup(html, 'html.parser')
            logger.info(f"P√°gina {url} acessada com sucesso")
        except requests.exceptions.HTTPError as e:
            # Se for erro 403 (Forbidden), tenta com mais t√©cnicas anti-bloqueio
            if e.response.status_code == 403:
                try:
                    # Tenta com um referrer de um grande site
                    session.headers.update({'Referer': 'https://www.google.com/'})
                    html = try_access_url_requests(url)
                    soup = BeautifulSoup(html, 'html.parser')
                    logger.info(f"P√°gina {url} acessada com referrer")
                except Exception:
                    # Tenta com HTTPS se HTTP falhar
                    if url.startswith('http://') and not url.startswith('https://'):
                        https_url = 'https://' + url[7:]
                        try:
                            html = try_access_url_requests(https_url)
                            soup = BeautifulSoup(html, 'html.parser')
                            logger.info(f"P√°gina {https_url} acessada via HTTPS")
                        except Exception as e2:
                            logger.error(f"Erro ao acessar {https_url}: {str(e2)}")
                            ids.append(f"Erro ao acessar o site (HTTPS): {str(e2)}")
                            return "\n".join(ids), "\n".join(links), technologies, contact_info
                    else:
                        logger.error(f"Erro 403 para {url}: {str(e)}")
                        ids.append(f"Erro ao acessar o site (403 Forbidden): {str(e)}")
                        return "\n".join(ids), "\n".join(links), technologies, contact_info
            # Se n√£o for 403, tenta com HTTPS
            elif url.startswith('http://') and not url.startswith('https://'):
                https_url = 'https://' + url[7:]
                try:
                    html = try_access_url_requests(https_url)
                    soup = BeautifulSoup(html, 'html.parser')
                    logger.info(f"P√°gina {https_url} acessada via HTTPS ap√≥s falha HTTP")
                except Exception as e2:
                    logger.error(f"Erro ao acessar {https_url}: {str(e2)}")
                    ids.append(f"Erro ao acessar o site (HTTPS): {str(e2)}")
                    return "\n".join(ids), "\n".join(links), technologies, contact_info
            else:
                logger.error(f"Erro HTTP para {url}: {str(e)}")
                ids.append(f"Erro ao acessar o site: {str(e)}")
                return "\n".join(ids), "\n".join(links), technologies, contact_info
        except Exception as e:
            logger.error(f"Erro geral ao acessar {url}: {str(e)}")
            ids.append(f"Erro ao acessar o site: {str(e)}")
            return "\n".join(ids), "\n".join(links), technologies, contact_info
        
        # Extrai o dom√≠nio base da URL para identificar links internos/externos
        base_domain = url.split('//')[-1].split('/')[0]
        logger.info(f"Iniciando extra√ß√£o de dados para {base_domain}")
        
        # Google Analytics (formato antigo UA-XXXXX-X)
        ga_pattern = re.compile(r'UA-[0-9]{4,10}-[0-9]{1,4}', re.IGNORECASE)
        ga_matches = ga_pattern.findall(html)
        logger.info(f"GA matches encontrados: {ga_matches}")
        if ga_matches:
            ids.append(f"Google Analytics (UA): {', '.join(set(ga_matches))}")
        
        # Google Analytics 4 - padr√£o mais espec√≠fico para IDs v√°lidos
        ga4_pattern = re.compile(r'G-[A-Z0-9]{10}(?![A-Z0-9])', re.IGNORECASE)
        ga4_matches = ga4_pattern.findall(html)
        # Filtrar apenas IDs que come√ßam com G- seguido de exatamente 10 caracteres alfanum√©ricos
        valid_ga4_matches = [match for match in ga4_matches if len(match) == 11 and match.startswith('G-')]
        logger.info(f"GA4 matches v√°lidos: {valid_ga4_matches}")
        if valid_ga4_matches:
            ids.append(f"Google Analytics 4 (GA4): {', '.join(set(valid_ga4_matches))}")
        
        # Google AdSense
        adsense_pattern = re.compile(r'pub-[0-9]+', re.IGNORECASE)
        adsense_matches = adsense_pattern.findall(html)
        if adsense_matches:
            ids.append(f"Google AdSense: {', '.join(set(adsense_matches))}")
        
        # Google Site Verification
        site_verification = soup.find('meta', attrs={'name': 'google-site-verification'})
        if site_verification:
            ids.append(f"Google Site Verification: {site_verification.get('content')}")
        
        # Microsoft Bing Verification
        msvalidate = soup.find('meta', attrs={'name': 'msvalidate.01'})
        if msvalidate:
            ids.append(f"Microsoft Bing Verification: {msvalidate.get('content')}")
        
        # Juicy Ad Code
        juicy_pattern = re.compile(r'juicy_code = \'([0-9]+)\'', re.IGNORECASE)
        juicy_matches = juicy_pattern.findall(html)
        if juicy_matches:
            ids.append(f"Juicy Ad Code: {', '.join(set(juicy_matches))}")
        
        # Facebook Pixel
        fb_pixel_pattern = re.compile(r'fbq\(\'init\', \'([0-9]+)\'\)', re.IGNORECASE)
        fb_pixel_matches = fb_pixel_pattern.findall(html)
        if fb_pixel_matches:
            ids.append(f"Facebook Pixel: {', '.join(set(fb_pixel_matches))}")
        
        # Google Tag Manager (formato GTM-XXXXXXX)
        gtm_pattern = re.compile(r'GTM-[A-Z0-9]{6,8}', re.IGNORECASE)
        gtm_matches = gtm_pattern.findall(html)
        if gtm_matches:
            ids.append(f"Google Tag Manager: {', '.join(set(gtm_matches))}")
        
        # Hotjar (formato mais espec√≠fico)
        hotjar_pattern = re.compile(r'hjid["\']?:\s*["\']?([0-9]{6,8})', re.IGNORECASE)
        hotjar_matches = hotjar_pattern.findall(html)
        if hotjar_matches:
            ids.append(f"Hotjar ID: {', '.join(set(hotjar_matches))}")
        
        # Detec√ß√£o avan√ßada de tecnologias usando Wappalyzer
        try:
            from Wappalyzer import Wappalyzer, WebPage
            
            # Cria uma inst√¢ncia do Wappalyzer
            wappalyzer = Wappalyzer.latest()
            
            # Cria um WebPage a partir do HTML obtido
            webpage = WebPage(url, html, {})
            
            # Analisa as tecnologias
            detected_technologies = wappalyzer.analyze_with_versions_and_categories(webpage)
            
            # Processa os resultados
            for tech_name, tech_info in detected_technologies.items():
                categories = tech_info.get('categories', [])
                versions = tech_info.get('versions', [])
                
                # Formata a informa√ß√£o da tecnologia
                tech_display = tech_name
                if versions:
                    tech_display += f" {versions[0]}"  # Mostra apenas a primeira vers√£o
                if categories:
                    tech_display += f" ({', '.join(categories[:2])})"  # Mostra at√© 2 categorias
                
                technologies.append(tech_display)
            
            logger.info(f"Wappalyzer detectou {len(detected_technologies)} tecnologias")
            
        except ImportError:
            logger.warning("Wappalyzer n√£o dispon√≠vel, usando detec√ß√£o manual")
            # Fallback para detec√ß√£o manual (c√≥digo original melhorado)
            
            # WordPress (padr√µes mais espec√≠ficos)
            if 'wp-content' in html or 'wp-includes' in html or 'wp-admin' in html or '/wp-json/' in html:
                technologies.append("WordPress")
            
            # Joomla (padr√µes mais espec√≠ficos)
            if 'joomla' in html.lower() or '/components/com_' in html or 'option=com_' in html:
                technologies.append("Joomla")
            
            # Drupal (padr√µes mais espec√≠ficos)
            if 'drupal' in html.lower() or 'sites/default/files' in html or '/modules/system/' in html:
                technologies.append("Drupal")
            
            # Bootstrap (vers√µes espec√≠ficas)
            bootstrap_patterns = [
                r'bootstrap[.-]([0-9]+\.[0-9]+)',
                r'bootstrap\.min\.css',
                r'bootstrap\.css'
            ]
            for pattern in bootstrap_patterns:
                if re.search(pattern, html, re.IGNORECASE):
                    technologies.append("Bootstrap")
                    break
            
            # jQuery (vers√µes espec√≠ficas)
            jquery_patterns = [
                r'jquery[.-]([0-9]+\.[0-9]+)',
                r'jquery\.min\.js',
                r'jquery\.js'
            ]
            for pattern in jquery_patterns:
                if re.search(pattern, html, re.IGNORECASE):
                    technologies.append("jQuery")
                    break
            
            # React (padr√µes mais espec√≠ficos)
            react_patterns = [
                r'_reactRootContainer',
                r'react[.-]([0-9]+\.[0-9]+)',
                r'react\.min\.js',
                r'ReactDOM',
                r'__REACT_DEVTOOLS_GLOBAL_HOOK__'
            ]
            for pattern in react_patterns:
                if re.search(pattern, html, re.IGNORECASE):
                    technologies.append("React")
                    break
            
            # Angular (padr√µes mais espec√≠ficos)
            angular_patterns = [
                r'ng-app',
                r'ng-controller',
                r'angular[.-]([0-9]+\.[0-9]+)',
                r'angular\.min\.js',
                r'@angular/core'
            ]
            for pattern in angular_patterns:
                if re.search(pattern, html, re.IGNORECASE):
                    technologies.append("Angular")
                    break
            
            # Vue.js (padr√µes mais espec√≠ficos)
            vue_patterns = [
                r'vue[.-]([0-9]+\.[0-9]+)',
                r'vue\.min\.js',
                r'new Vue\(',
                r'v-if=',
                r'v-for=',
                r'__VUE__'
            ]
            for pattern in vue_patterns:
                if re.search(pattern, html, re.IGNORECASE):
                    technologies.append("Vue.js")
                    break
            
            # Tecnologias adicionais
            
            # PHP
            if re.search(r'\.php', html, re.IGNORECASE) or 'X-Powered-By: PHP' in str(response.headers if 'response' in locals() else ''):
                technologies.append("PHP")
            
            # Laravel
            if 'laravel' in html.lower() or 'laravel_session' in html.lower():
                technologies.append("Laravel")
            
            # Django
            if 'django' in html.lower() or 'csrfmiddlewaretoken' in html.lower():
                technologies.append("Django")
            
            # Node.js
            if 'node' in html.lower() or 'nodejs' in html.lower() or 'express' in html.lower():
                technologies.append("Node.js")
            
            # Shopify
            if 'shopify' in html.lower() or 'cdn.shopify.com' in html:
                technologies.append("Shopify")
            
            # WooCommerce
            if 'woocommerce' in html.lower() or 'wc-' in html:
                technologies.append("WooCommerce")
            
            # Magento
            if 'magento' in html.lower() or 'mage/cookies.js' in html:
                technologies.append("Magento")
            
        except Exception as e:
            logger.error(f"Erro na detec√ß√£o de tecnologias com Wappalyzer: {e}")
            logger.info("Usando detec√ß√£o manual como fallback")
            
            # Fallback b√°sico em caso de erro
            if 'wp-content' in html or 'wp-includes' in html:
                technologies.append("WordPress")
            if 'jquery' in html.lower():
                technologies.append("jQuery")
            if 'bootstrap' in html.lower():
                technologies.append("Bootstrap")
        
        # Extra√ß√£o de informa√ß√µes de contato
        # E-mails
        email_pattern = re.compile(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}', re.IGNORECASE)
        email_matches = email_pattern.findall(html)
        if email_matches:
            unique_emails = list(set(email_matches))
            contact_info.append(f"E-mails: {', '.join(unique_emails[:5])}")
            if len(unique_emails) > 5:
                contact_info.append(f"... e mais {len(unique_emails) - 5} e-mails")
        
        # Telefones (formato brasileiro)
        phone_pattern = re.compile(r'\(?\d{2}\)?\s?\d{4,5}-?\d{4}', re.IGNORECASE)
        phone_matches = phone_pattern.findall(html)
        if phone_matches:
            unique_phones = list(set(phone_matches))
            contact_info.append(f"Telefones: {', '.join(unique_phones[:5])}")
            if len(unique_phones) > 5:
                contact_info.append(f"... e mais {len(unique_phones) - 5} telefones")
        
        # Extra√ß√£o de links
        for link in soup.find_all('a', href=True):
            href = link.get('href')
            if href and not href.startswith('#') and not href.startswith('javascript:'):
                # Normaliza o link
                if href.startswith('/'):
                    href = url.rstrip('/') + href
                elif not href.startswith(('http://', 'https://', 'ftp://')):
                    href = url.rstrip('/') + '/' + href
                
                # Categoriza o link
                if any(social in href.lower() for social in ['facebook.com', 'twitter.com', 'instagram.com', 'linkedin.com', 'youtube.com', 'pinterest.com', 'tiktok.com']):
                    social_links.append(href)
                elif base_domain in href:
                    internal_links.append(href)
                else:
                    external_links.append(href)
        
        # Adiciona links categorizados
        if social_links:
            links.append("Links de Redes Sociais:")
            links.extend(social_links[:10])
            if len(social_links) > 10:
                links.append(f"... e mais {len(social_links) - 10} links de redes sociais")
        
        if external_links:
            links.append("\nLinks Externos:")
            links.extend(external_links[:15])
            if len(external_links) > 15:
                links.append(f"... e mais {len(external_links) - 15} links externos")
        
        if internal_links:
            links.append("\nLinks Internos:")
            links.extend(internal_links[:10])
            if len(internal_links) > 10:
                links.append(f"... e mais {len(internal_links) - 10} links internos")
    
    except Exception as e:
        ids.append(f"Erro ao acessar o site: {str(e)}")
    
    return "\n".join(ids), "\n".join(links), technologies, contact_info


def check_ssl(dominio_analisado):
    """
    Verifica informa√ß√µes do certificado SSL do dom√≠nio com seguran√ßa aprimorada
    
    Args:
        dominio_analisado (str): O dom√≠nio para verificar o certificado SSL
        
    Returns:
        str: Informa√ß√µes formatadas do certificado SSL ou mensagem de erro
    """
    try:
        # Primeiro, verificar se o dom√≠nio responde na porta 443
        try:
            # Tentar conectar com timeout de 5 segundos
            sock = socket.create_connection((dominio_analisado, 443), timeout=5)
            sock.close()
        except (socket.timeout, socket.error) as e:
            return f"Certificado SSL: N√£o dispon√≠vel (o servidor n√£o responde na porta 443)"
        
        # Configurar contexto SSL seguro (modo cliente padr√£o)
        context = ssl.create_default_context()
        # Manter verifica√ß√µes de seguran√ßa habilitadas por padr√£o
        # context.check_hostname = True (padr√£o)
        # context.verify_mode = ssl.CERT_REQUIRED (padr√£o)
        
        # Tentar obter o certificado com verifica√ß√£o segura
        try:
            with socket.create_connection((dominio_analisado, 443), timeout=5) as sock:
                with context.wrap_socket(sock, server_hostname=dominio_analisado) as ssock:
                    cert = ssock.getpeercert(binary_form=False)
                    
                    # Extrair informa√ß√µes relevantes
                    issued_to = cert.get('subject', [])
                    issued_by = cert.get('issuer', [])
                    valid_from = cert.get('notBefore', '')
                    valid_until = cert.get('notAfter', '')
                    
                    # Formatar sa√≠da
                    ssl_info = []
                    ssl_info.append(f"Certificado SSL: V√°lido e Verificado ‚úì")
                    
                    # Emissor
                    issuer_cn = None
                    for item in issued_by:
                        if item[0][0] == 'commonName':
                            issuer_cn = item[0][1]
                    if issuer_cn:
                        ssl_info.append(f"Emitido por: {issuer_cn}")
                    
                    # Validade
                    if valid_from and valid_until:
                        ssl_info.append(f"V√°lido de: {valid_from}")
                        ssl_info.append(f"V√°lido at√©: {valid_until}")
                        
                        # Verificar se est√° pr√≥ximo de expirar
                        try:
                            expiry_date = ssl.cert_time_to_seconds(valid_until)
                            current_time = datetime.datetime.now().timestamp()
                            days_remaining = int((expiry_date - current_time) / 86400)
                            ssl_info.append(f"Dias restantes: {days_remaining}")
                            
                            # Alerta se pr√≥ximo do vencimento
                            if days_remaining <= 30:
                                ssl_info.append(f"‚ö†Ô∏è ATEN√á√ÉO: Certificado expira em menos de 30 dias!")
                        except Exception as e:
                            ssl_info.append(f"N√£o foi poss√≠vel calcular dias restantes: {str(e)}")
                    
                    # Dom√≠nios alternativos (SAN)
                    alt_names = []
                    for ext in cert.get('subjectAltName', []):
                        if ext[0].lower() == 'dns':
                            alt_names.append(ext[1])
                    if alt_names:
                        ssl_info.append(f"Nomes alternativos: {', '.join(alt_names[:5])}")
                        if len(alt_names) > 5:
                            ssl_info.append(f"... e mais {len(alt_names) - 5} dom√≠nios")
                    
                    return "\n".join(ssl_info)
        except ssl.SSLCertVerificationError as e:
            # Certificado presente mas n√£o verific√°vel - tentar obter informa√ß√µes b√°sicas
            try:
                # Usar contexto menos restritivo apenas para obter informa√ß√µes do certificado
                insecure_context = ssl.create_default_context()
                insecure_context.check_hostname = False
                insecure_context.verify_mode = ssl.CERT_NONE
                
                with socket.create_connection((dominio_analisado, 443), timeout=5) as sock:
                    with insecure_context.wrap_socket(sock, server_hostname=dominio_analisado) as ssock:
                        cert = ssock.getpeercert(binary_form=False)
                        
                        ssl_info = []
                        ssl_info.append(f"Certificado SSL: Presente mas N√ÉO VERIFICADO ‚ö†Ô∏è")
                        ssl_info.append(f"Erro de verifica√ß√£o: {str(e)}")
                        
                        # Ainda assim, extrair informa√ß√µes b√°sicas
                        issued_by = cert.get('issuer', [])
                        valid_until = cert.get('notAfter', '')
                        
                        issuer_cn = None
                        for item in issued_by:
                            if item[0][0] == 'commonName':
                                issuer_cn = item[0][1]
                        if issuer_cn:
                            ssl_info.append(f"Emitido por: {issuer_cn}")
                        
                        if valid_until:
                            ssl_info.append(f"V√°lido at√©: {valid_until}")
                        
                        return "\n".join(ssl_info)
            except Exception:
                return f"Certificado SSL: Erro de verifica√ß√£o - {str(e)}"
        except ssl.SSLError as e:
            return f"Certificado SSL: Erro SSL ({str(e)})"
    except socket.gaierror as e:
        return f"Certificado SSL: Erro de resolu√ß√£o de nome ({str(e)})"
    except socket.timeout as e:
        return f"Certificado SSL: Timeout na conex√£o ({str(e)})"
    except Exception as e:
        return f"Certificado SSL: N√£o dispon√≠vel ou erro ({str(e)})"

def check_dns_records(dominio_analisado):
    """
    Verifica registros DNS do dom√≠nio de forma otimizada
    
    Args:
        dominio_analisado (str): O dom√≠nio para consultar registros DNS
        
    Returns:
        str: Informa√ß√µes formatadas dos registros DNS encontrados
    """
    dns_info = []
    
    # Tipos de registros a verificar
    record_types = ['A', 'AAAA', 'MX', 'NS', 'TXT', 'CNAME']
    
    # Configurar resolvedores DNS alternativos para melhorar a confiabilidade
    resolvers = [
        '8.8.8.8',        # Google DNS
        '1.1.1.1',        # Cloudflare DNS
        '208.67.222.222', # OpenDNS
        '200.160.0.10'    # DNS do Registro.br (para dom√≠nios .br)
    ]
    
    # Criar um resolvedor personalizado configurado uma √∫nica vez
    resolver = dns.resolver.Resolver()
    resolver.nameservers = resolvers  # Usar todos os resolvers
    resolver.timeout = 5.0  # Timeout em segundos
    resolver.lifetime = 10.0  # Tempo m√°ximo para uma consulta
    
    domain_exists = False
    
    # Consultar cada tipo de registro
    for record_type in record_types:
        try:
            # Tentar resolver com todos os nameservers configurados
            answers = resolver.query(dominio_analisado, record_type)
            records = []
            
            for rdata in answers:
                if record_type == 'MX':
                    records.append(f"{rdata.preference} {rdata.exchange}")
                else:
                    records.append(str(rdata))
            
            if records:
                dns_info.append(f"Registros {record_type}:")
                for record in records:
                    dns_info.append(f"  {record}")
                domain_exists = True
                logger.debug(f"Registro {record_type} encontrado para {dominio_analisado}")
        except dns.resolver.NoAnswer:
            # Sem resposta para este tipo de registro, mas dom√≠nio existe
            domain_exists = True
            logger.debug(f"NoAnswer para {record_type} em {dominio_analisado}")
            continue
        except dns.resolver.NXDOMAIN:
            # Dom√≠nio n√£o existe - erro definitivo
            logger.warning(f"NXDOMAIN para {dominio_analisado}")
            return f"Erro: O dom√≠nio '{dominio_analisado}' n√£o existe ou n√£o est√° registrado"
        except dns.resolver.NoNameservers:
            # Nenhum servidor de nomes dispon√≠vel
            logger.warning(f"NoNameservers para {record_type} em {dominio_analisado}")
            continue
        except dns.resolver.Timeout:
            # Timeout na consulta
            logger.warning(f"Timeout para {record_type} em {dominio_analisado}")
            continue
        except Exception as e:
            # Outros erros
            logger.error(f"Erro ao consultar {record_type} para {dominio_analisado}: {str(e)}")
            continue
    
    # An√°lise dos resultados
    if not dns_info:
        if not domain_exists:
            return f"Erro: N√£o foi poss√≠vel obter registros DNS para '{dominio_analisado}'. Verifique a conectividade ou se o dom√≠nio existe"
        else:
            return f"Aviso: O dom√≠nio '{dominio_analisado}' existe, mas n√£o possui registros DNS p√∫blicos vis√≠veis"
    
    return "\n".join(dns_info)


def analyze(dominio_analisado):
    """
    Fun√ß√£o principal que realiza an√°lise completa de um dom√≠nio.
    
    Args:
        dominio_analisado (str): O dom√≠nio a ser analisado
        
    Returns:
        tuple: Tupla contendo todas as informa√ß√µes coletadas:
            - ntpbr_time: Hor√°rio oficial do Brasil
            - whois: Informa√ß√µes WHOIS do dom√≠nio
            - ssl_info: Informa√ß√µes do certificado SSL
            - dns_info: Registros DNS
            - servidores_list: Lista de subdom√≠nios encontrados
            - ids: C√≥digos de identifica√ß√£o encontrados
            - links_encontrados: Links extra√≠dos do site
            - technologies: Tecnologias detectadas
            - contact_info: Informa√ß√µes de contato
    """
    ntpservs = ['a.st1.ntp.br',
                'b.st1.ntp.br',
                'c.st1.ntp.br',
                'd.st1.ntp.br']

    ntpbr_time = ntp_time(ntpservs)

    dicsubdominios = {}
    servidores = findservidor(dominio_analisado, dicsubdominios)

    whois = d_whois(dominio_analisado)

    whois_subdomains = ip_whois(dicsubdominios)
    
    # Usa a fun√ß√£o get_ids local (sem importa√ß√£o relativa)
    ids, links_encontrados, technologies, contact_info = get_ids(dominio_analisado)
    
    # Novas informa√ß√µes
    ssl_info = check_ssl(dominio_analisado)
    dns_info = check_dns_records(dominio_analisado)

    # Prepara a lista de servidores como tuplas (nome, ip, whois)
    servidores_list = [(k, v, whois_subdomains.get(k, "Informa√ß√£o n√£o dispon√≠vel")) for k, v in servidores.items()]
    
    # Retorna uma tupla com 9 valores na ordem esperada pela view
    return (
        ntpbr_time,           # 0
        whois,                # 1
        ssl_info,             # 2
        dns_info,             # 3
        servidores_list,      # 4
        ids,                  # 5
        links_encontrados,    # 6
        technologies,         # 7
        contact_info          # 8
    )

